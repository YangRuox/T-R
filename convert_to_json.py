# convert_to_json_fixed.py
import json
import numpy as np
import joblib
import pandas as pd
import os
from sklearn.preprocessing import StandardScaler

def load_multilingual_jobs():
    """åŠ è½½æ‰€æœ‰è¯­è¨€çš„èŒä¸šåç§°"""
    print("Loading multilingual job files...")
    
    languages = ['en', 'zh', 'es', 'fr', 'ru', 'ar']
    job_translations = {}
    
    for lang in languages:
        filepath = f"job_{lang}.npy"
        try:
            data = np.load(filepath, allow_pickle=True)
            job_list = data.tolist()
            job_translations[lang] = job_list
            print(f"  âœ… {filepath} - {len(job_list)} jobs")
            
            # æ˜¾ç¤ºç¤ºä¾‹
            if len(job_list) > 0:
                print(f"    ç¤ºä¾‹: {job_list[:3]}")
        except FileNotFoundError:
            print(f"  âŒ {filepath} not found")
            job_translations[lang] = []
    
    # éªŒè¯æ‰€æœ‰è¯­è¨€æ–‡ä»¶é•¿åº¦ä¸€è‡´
    lengths = [len(job_translations[lang]) for lang in languages if lang in job_translations]
    if lengths and len(set(lengths)) > 1:
        print(f"âš ï¸ è­¦å‘Š: è¯­è¨€æ–‡ä»¶é•¿åº¦ä¸ä¸€è‡´: {lengths}")
    
    return job_translations

def load_base_job_data():
    """åŠ è½½åŸºç¡€èŒä¸šæ•°æ®"""
    print("\nLoading base job data...")
    
    base_data = {}
    
    # åŠ è½½èŒä¸šä»£ç 
    try:
        job_codes = np.load("job_codes.npy", allow_pickle=True).tolist()
        base_data['job_codes'] = job_codes
        print(f"  âœ… job_codes.npy - {len(job_codes)} codes")
        print(f"    ç¤ºä¾‹: {job_codes[:5]}")
    except FileNotFoundError:
        print("  âŒ job_codes.npy not found")
        base_data['job_codes'] = []
    
    # åŠ è½½PCAæƒé‡
    try:
        pca_weights = np.load("pca_weights.npy", allow_pickle=True).tolist()
        base_data['pca_weights'] = pca_weights
        print(f"  âœ… pca_weights.npy - shape: {np.array(pca_weights).shape if pca_weights else 'N/A'}")
    except FileNotFoundError:
        print("  âŒ pca_weights.npy not found")
        base_data['pca_weights'] = []
    
    # åŠ è½½èŒä¸šç‰¹å¾
    try:
        scaled_features = np.load("scaled_job_features.npy", allow_pickle=True).tolist()
        base_data['scaled_features'] = scaled_features
        print(f"  âœ… scaled_job_features.npy - {len(scaled_features)} jobs")
    except FileNotFoundError:
        print("  âŒ scaled_job_features.npy not found")
        base_data['scaled_features'] = []
    
    return base_data

def load_questions():
    """åŠ è½½é—®é¢˜æ•°æ®"""
    print("\nLoading questions.tsv...")
    
    languages = ['en', 'zh', 'es', 'fr', 'ru', 'ar']
    questions_dict = {}
    
    try:
        questions_df = pd.read_csv('questions.tsv', sep='\t')
        print(f"  âœ… Loaded questions.tsv - {len(questions_df)} questions")
        
        for lang in languages:
            if lang in questions_df.columns:
                questions = questions_df[lang].fillna('').astype(str).tolist()
                questions_dict[lang] = questions
                print(f"    âœ… {lang}: {len(questions)} questions")
            else:
                print(f"    âš ï¸ {lang} not found, using English")
                questions_dict[lang] = questions_df['en'].fillna('').astype(str).tolist()
        
        return questions_dict, len(questions_df)
    
    except FileNotFoundError:
        print("âŒ questions.tsv not found")
        return {}, 0

def convert_tsv_files():
    """å°†æ‰€æœ‰çš„.tsvæ–‡ä»¶è½¬æ¢ä¸ºJSONæ ¼å¼"""
    print("\nConverting .tsv files...")
    
    tsv_data = {}
    
    try:
        # åŠ è½½meanNorms.tsv
        mean_norms = pd.read_csv('meanNorms.tsv', sep='\t')
        mean_norms_json = {}
        for idx, row in mean_norms.iterrows():
            group = int(row['group'])
            values = row[1:].tolist()
            mean_norms_json[group] = values
        tsv_data['mean_norms'] = mean_norms_json
        print(f"  âœ… meanNorms.tsv - {len(mean_norms_json)} groups")
    except FileNotFoundError:
        print("  âŒ meanNorms.tsv not found")
        tsv_data['mean_norms'] = {}
    
    try:
        # åŠ è½½sdNorms.tsv
        sd_norms = pd.read_csv('sdNorms.tsv', sep='\t')
        sd_norms_json = {}
        for idx, row in sd_norms.iterrows():
            group = int(row['group'])
            values = row[1:].tolist()
            sd_norms_json[group] = values
        tsv_data['sd_norms'] = sd_norms_json
        print(f"  âœ… sdNorms.tsv - {len(sd_norms_json)} groups")
    except FileNotFoundError:
        print("  âŒ sdNorms.tsv not found")
        tsv_data['sd_norms'] = {}
    
    try:
        # åŠ è½½weightsB5.tsv
        weights = pd.read_csv('weightsB5.tsv', sep='\t')
        tsv_data['weights'] = weights.values.tolist()
        print(f"  âœ… weightsB5.tsv - {len(tsv_data['weights'])} questions Ã— 5 traits")
    except FileNotFoundError:
        print("  âŒ weightsB5.tsv not found")
        tsv_data['weights'] = []
    
    return tsv_data

def load_scaler():
    """åŠ è½½å’Œè½¬æ¢scaler"""
    print("\nLoading scaler...")
    
    try:
        scaler = joblib.load("your_scaler.pkl")
        
        scaler_params = {
            "mean": scaler.mean_.tolist() if hasattr(scaler, 'mean_') else [],
            "scale": scaler.scale_.tolist() if hasattr(scaler, 'scale_') else [],
            "var": scaler.var_.tolist() if hasattr(scaler, 'var_') else [],
            "n_samples_seen": int(scaler.n_samples_seen_) if hasattr(scaler, 'n_samples_seen_') else 0,
            "feature_names": ["Neuroticism", "Extraversion", "Openness", "Agreeableness", "Conscientiousness"]
        }
        
        print("  âœ… your_scaler.pkl loaded")
        return scaler_params
    
    except FileNotFoundError:
        print("  âŒ your_scaler.pkl not found")
        return {}

def load_other_files():
    """åŠ è½½å…¶ä»–æ”¯æŒæ–‡ä»¶"""
    print("\nLoading other support files...")
    
    other_data = {}
    
    try:
        text_dict = np.load("text_dict.npy", allow_pickle=True).item()
        other_data['text_dict'] = text_dict
        print("  âœ… text_dict.npy loaded")
    except:
        print("  âŒ text_dict.npy not found")
        other_data['text_dict'] = {}
    
    try:
        language_display = np.load("language_display.npy", allow_pickle=True).item()
        other_data['language_display'] = language_display
        print("  âœ… language_display.npy loaded")
    except:
        print("  âŒ language_display.npy not found")
        other_data['language_display'] = {}
    
    return other_data

def get_translation_texts():
    """è·å–æ‰€æœ‰ç¿»è¯‘æ–‡æœ¬"""
    return {
        "titles": {
            "en": "Your Big Five Personality Profile (T scores)",
            "zh": "ä½ çš„å¤§äº”äººæ ¼é›·è¾¾å›¾ï¼ˆTåˆ†ï¼‰",
            "es": "Tu Perfil de Personalidad Big Five (Puntajes T)",
            "fr": "Votre profil de personnalitÃ© Big Five (scores T)",
            "ru": "Ğ’Ğ°Ñˆ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»ÑŒ Ğ»Ğ¸Ñ‡Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ¿Ğ¾ Big Five (T-Ğ±Ğ°Ğ»Ğ»Ñ‹)",
            "ar": "Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ (Big Five) Ø¨Ø¯Ø±Ø¬Ø§Øª T",
        },
        "trait_names": {
            "en": ["Neuroticism", "Extraversion", "Openness", "Agreeableness", "Conscientiousness"],
            "zh": ["ç¥ç»è´¨", "å¤–å‘æ€§", "å¼€æ”¾æ€§", "å®œäººæ€§", "å°½è´£æ€§"],
            "es": ["Neuroticismo", "ExtraversiÃ³n", "Apertura", "Amabilidad", "Conciencia"],
            "fr": ["NÃ©vrosisme", "Extraversion", "Ouverture", "AmabilitÃ©", "ConscienciositÃ©"],
            "ru": ["ĞĞµĞ²Ñ€Ğ¾Ñ‚Ğ¸Ğ·Ğ¼", "Ğ­ĞºÑÑ‚Ñ€Ğ°Ğ²ĞµÑ€ÑĞ¸Ñ", "ĞÑ‚ĞºÑ€Ñ‹Ñ‚Ğ¾ÑÑ‚ÑŒ", "Ğ”Ğ¾Ğ±Ñ€Ğ¾Ğ¶ĞµĞ»Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ", "Ğ¡Ğ¾Ğ·Ğ½Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ"],
            "ar": ["Ø§Ù„Ø¶ÙŠÙ‚ Ø§Ù„Ø¹ØµØ¨ÙŠ", "Ø§Ù„Ø§Ù†ÙØªØ§Ø­", "Ø§Ù„Ø§Ù†Ø¨Ø³Ø§Ø·ÙŠØ©", "Ø§Ù„ØªØ¹Ø§Ø·Ù", "Ø§Ù„Ø¶Ù…ÙŠØ± Ø§Ù„Ù…Ù‡Ù†ÙŠ"]
        },
        "disclaimer": {
            "en": "The career recommendations provided here are for your reference only. It's important to consider your personal circumstances, preferences, and goals when making a career decision. We encourage you to explore different options and take the time to evaluate each one carefully. May you find a fulfilling and rewarding career that aligns with your values and aspirations. Best of luck on your journey to success! ğŸ˜Š",
            "zh": "è¿™é‡Œæä¾›çš„èŒä¸šæ¨èä»…ä¾›å‚è€ƒã€‚åœ¨åšå‡ºèŒä¸šé€‰æ‹©æ—¶ï¼Œè¯·åŠ¡å¿…è€ƒè™‘æ‚¨çš„ä¸ªäººæƒ…å†µã€å…´è¶£å’Œç›®æ ‡ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨æ¢ç´¢ä¸åŒçš„èŒä¸šé€‰é¡¹ï¼Œå¹¶ä»”ç»†è¯„ä¼°æ¯ä¸€ä¸ªé€‰æ‹©ã€‚å¸Œæœ›æ‚¨èƒ½å¤Ÿæ‰¾åˆ°ä¸€ä¸ªç¬¦åˆè‡ªå·±ä»·å€¼è§‚å’Œäººç”Ÿç›®æ ‡çš„ç†æƒ³èŒä¸šï¼Œç¥æ‚¨åœ¨èŒä¸šç”Ÿæ¶¯ä¸­å–å¾—åœ†æ»¡æˆåŠŸï¼ğŸ˜Š",
            "es": "Las recomendaciones de carrera proporcionadas aquÃ­ son solo para su referencia. Es importante tener en cuenta sus circunstancias personales, preferencias y objetivos al tomar una decisiÃ³n sobre su carrera. Le animamos a explorar diferentes opciones y tomarse el tiempo necesario para evaluar cada una de ellas con cuidado. Â¡Le deseamos mucho Ã©xito en su camino hacia una carrera gratificante y satisfactoria! ğŸ˜Š",
            "fr": "Les recomendaciones de carriÃ¨re fournies aquÃ­ son uniquement Ã  titre de rÃ©fÃ©rence. Il est important de prendre en compte vos circonstances personnelles, vos prÃ©fÃ©rences et vos objectifs lorsque vous prenez una dÃ©cision concernant votre carriÃ¨re. Nous vous encourageons Ã  explorer diferentes opciones et Ã  prendre le temps d'Ã©valuer cada choix avec soin. Nous vous souhaitons de trouver une carriÃ¨re Ã©panouissante et gratifiante qui corresponde Ã  vos valeurs et aspirations. Bonne chance dans votre parcours vers le succÃ¨s ! ğŸ˜Š",
            "ru": "ĞŸÑ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ¿Ğ¾ ĞºĞ°Ñ€ÑŒĞµÑ€Ğµ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ Ğ²Ğ°ÑˆĞµĞ³Ğ¾ Ğ¾Ğ·Ğ½Ğ°ĞºĞ¾Ğ¼Ğ»ĞµĞ½Ğ¸Ñ. Ğ’Ğ°Ğ¶Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°ÑˆĞ¸ Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°, Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ ĞºĞ°Ñ€ÑŒĞµÑ€Ğµ. ĞœÑ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ²Ğ°Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¸ ÑƒĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸Ğ· Ğ½Ğ¸Ñ…. Ğ–ĞµĞ»Ğ°ĞµĞ¼ Ğ²Ğ°Ğ¼ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ ĞºĞ°Ñ€ÑŒĞµÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ±ÑƒĞ´ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°ÑˆĞ¸Ğ¼ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¸ ÑƒÑÑ‚Ñ€ĞµĞ¼Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¿ÑƒÑ‚Ğ¸ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ! ğŸ˜Š",
            "ar": "Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ù‡Ù†ÙŠØ© Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ğ·Ğ´ĞµÑÑŒ Ğ¿Ñ€ĞµĞ´Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ñ‹ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ´Ğ»Ñ ÑĞ¿Ñ€Ğ°Ğ²ĞºĞ¸. Ğ’Ğ°Ğ¶Ğ½Ğ¾ ÑƒÑ‡Ğ¸Ñ‚Ñ‹Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°ÑˆĞ¸ Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ¾Ğ±ÑÑ‚Ğ¾ÑÑ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°, Ğ¿Ñ€ĞµĞ´Ğ¿Ğ¾Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ Ğ¸ Ñ†ĞµĞ»Ğ¸ Ğ¿Ñ€Ğ¸ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚Ğ¸Ğ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ñ Ğ¾ ĞºĞ°Ñ€ÑŒĞµÑ€Ğµ. ĞœÑ‹ Ğ¿Ñ€Ğ¸Ğ·Ñ‹Ğ²Ğ°ĞµĞ¼ Ğ²Ğ°Ñ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ½Ñ‚Ñ‹ Ğ¸ ÑƒĞ´ĞµĞ»ÑÑ‚ÑŒ Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ Ğ½Ğ° Ñ‚Ñ‰Ğ°Ñ‚ĞµĞ»ÑŒĞ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ¸Ğ· Ğ½Ğ¸Ñ…. Ğ–ĞµĞ»Ğ°ĞµĞ¼ Ğ²Ğ°Ğ¼ Ğ½Ğ°Ğ¹Ñ‚Ğ¸ ĞºĞ°Ñ€ÑŒĞµÑ€Ñƒ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ğ°Ñ Ğ±ÑƒĞ´ĞµÑ‚ ÑĞ¾Ğ¾Ñ‚Ğ²ĞµÑ‚ÑÑ‚Ğ²Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²Ğ°ÑˆĞ¸Ğ¼ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑĞ¼ Ğ¸ ÑƒÑÑ‚Ñ€ĞµĞ¼Ğ»ĞµĞ½Ğ¸ÑĞ¼, Ğ¸ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ³Ğ¾ Ğ¿ÑƒÑ‚Ğ¸ Ğº ÑƒÑĞ¿ĞµÑ…Ñƒ! ğŸ˜Š"
        },
        "ideal_job_prompt": {
            "en": "Please enter your ideal career (e.g., Data Scientist):",
            "zh": "è¯·è¾“å…¥æ‚¨çš„ç†æƒ³èŒä¸šï¼ˆä¾‹å¦‚ï¼šæ•°æ®ç§‘å­¦å®¶ï¼‰ï¼š",
            "es": "Por favor, introduzca su carrera ideal (por ejemplo: CientÃ­fico de datos):",
            "fr": "Veuillez saisir votre mÃ©tier idÃ©al (par exemple : Data Scientist) :",
            "ru": "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ²Ğ°ÑˆÑƒ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ñ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€: ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»Ğ¸ÑÑ‚ Ğ¿Ğ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼):",
            "ar": "ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù…Ù‡Ù†ØªÙƒ Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ© (Ù…Ø«Ø§Ù„: Ø¹Ø§Ù„Ù… Ø¨ÙŠØ§Ù†Ø§Øª):"
        },
        "ideal_job_warning": {
            "en": "âš ï¸ Please enter your ideal career.",
            "zh": "âš ï¸ è¯·è¾“å…¥æ‚¨çš„ç†æƒ³èŒä¸šã€‚",
            "es": "âš ï¸ Por favor, introduzca su carrera ideal.",
            "fr": "âš ï¸ Veuillez saisir votre mÃ©tier idÃ©al.",
            "ru": "âš ï¸ ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ²Ğ²ĞµĞ´Ğ¸Ñ‚Ğµ Ğ²Ğ°ÑˆÑƒ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½ÑƒÑ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ñ.",
            "ar": "âš ï¸ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù…Ù‡Ù†ØªÙƒ Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©."
        },
        "ideal_job_result": {
            "en": "The career closest to your ideal is: **{}**",
            "zh": "æ‚¨çš„ç†æƒ³èŒä¸šæœ€ç›¸è¿‘çš„æ˜¯ï¼š**{}**",
            "es": "La carrera mÃ¡s cercana a su ideal es: **{}**",
            "fr": "Le mÃ©tier le plus proche de votre idÃ©al est : **{}**",
            "ru": "Ğ¡Ğ°Ğ¼Ğ°Ñ Ğ±Ğ»Ğ¸Ğ·ĞºĞ°Ñ Ğº Ğ²Ğ°ÑˆĞµĞ¹ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ñ: **{}**",
            "ar": "Ø£Ù‚Ø±Ø¨ Ù…Ù‡Ù†Ø© Ğº Ğ²Ğ°ÑˆĞµĞ¹ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸: **{}**"
        },
        "closest_text": {
            "en": "Your trait closest to the ideal career:",
            "zh": "ä¸ç†æƒ³èŒä¸šç‰¹å¾æœ€æ¥è¿‘çš„æ˜¯ï¼š", 
            "es": "Tu rasgo mÃ¡s cercano al trabajo ideal:",
            "fr": "Votre trait le plus proche du mÃ©tier idÃ©al :",
            "ru": "Ğ’Ğ°ÑˆĞ° Ñ‡ĞµÑ€Ñ‚Ğ°, Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ±Ğ»Ğ¸Ğ·ĞºĞ°Ñ Ğº Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸:",
            "ar": "Ø³Ù…ØªÙƒ Ø§Ù„Ø£Ù‚Ø±Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©:"
        },
        "furthest_text": {
            "en": "Your trait furthest from the ideal career:",
            "zh": "ä¸ç†æƒ³èŒä¸šç‰¹å¾å·®è·æœ€å¤§çš„æ˜¯ï¼š",
            "es": "Tu rasgo mÃ¡s alejado del trabajo ideal:",
            "fr": "Votre trait le plus Ã©loignÃ© du mÃ©tier idÃ©al :", 
            "ru": "Ğ’Ğ°ÑˆĞ° Ñ‡ĞµÑ€Ñ‚Ğ°, Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ´Ğ°Ğ»Ñ‘ĞºĞ°Ñ Ğ¾Ñ‚ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¸:",
            "ar": "Ø³Ù…ØªÙƒ Ø§Ù„Ø£Ø¨Ø¹Ø¯ Ø¹Ù† Ø§Ù„Ù…Ù‡Ù†Ø© Ø§Ù„Ù…Ø«Ø§Ù„ÙŠØ©:"
        }
    }

def get_metadata():
    """è·å–å…ƒæ•°æ®"""
    return {
        "version": "2.0.0",
        "created_at": pd.Timestamp.now().isoformat(),
        "data_sources": [
            "job_en.npy, job_zh.npy, job_es.npy, job_fr.npy, job_ru.npy, job_ar.npy",
            "job_codes.npy",
            "scaled_job_features.npy",
            "pca_weights.npy",
            "meanNorms.tsv",
            "sdNorms.tsv",
            "questions.tsv",
            "weightsB5.tsv",
            "your_scaler.pkl"
        ],
        "languages_supported": ['en', 'zh', 'es', 'fr', 'ru', 'ar'],
        "n_jobs": 263,
        "model_info": {
            "name": "JobRecommenderMLP",
            "input_dim": 5,
            "hidden_dim": 128,
            "output_dim": 263
        }
    }

def convert_all_data():
    """ä¸»å‡½æ•°ï¼šè½¬æ¢æ‰€æœ‰æ•°æ®åˆ°JSONæ ¼å¼"""
    print("=" * 60)
    print("Converting all data to JSON format...")
    print("=" * 60)
    
    try:
        # 1. åŠ è½½å¤šè¯­è¨€èŒä¸šåç§°
        job_translations = load_multilingual_jobs()
        
        # 2. åŠ è½½åŸºç¡€èŒä¸šæ•°æ®
        base_data = load_base_job_data()
        
        # 3. åŠ è½½å…¶ä»–æ–‡ä»¶
        other_data = load_other_files()
        
        # 4. åŠ è½½é—®é¢˜æ•°æ®
        questions_dict, n_questions = load_questions()
        
        # 5. åŠ è½½TSVæ•°æ®
        tsv_data = convert_tsv_files()
        
        # 6. åŠ è½½Scaler
        scaler_params = load_scaler()
        
        # 7. è·å–ç¿»è¯‘æ–‡æœ¬
        translations = get_translation_texts()
        
        # 8. æ„å»ºå®Œæ•´çš„æ•°æ®ç»“æ„
        print("\n" + "=" * 60)
        print("Building complete data structure...")
        print("=" * 60)
        
        complete_data = {
            # èŒä¸šæ•°æ® - å…³é”®éƒ¨åˆ†
            "job_translations": job_translations,  # å¤šè¯­è¨€èŒä¸šåç§°
            "job_codes": base_data.get('job_codes', []),
            "pca_weights": base_data.get('pca_weights', []),
            "scaled_features": base_data.get('scaled_features', []),
            
            # å‘åå…¼å®¹ï¼šä½¿ç”¨è‹±æ–‡ä½œä¸ºé»˜è®¤èŒä¸šåç§°
            "job_names": job_translations.get('en', []),
            
            # å…¶ä»–æ•°æ®
            **other_data,
            
            # é—®é¢˜æ•°æ®
            "questions": questions_dict,
            
            # æ ‡å‡†åŒ–æ•°æ®
            **tsv_data,
            
            # Scalerå‚æ•°
            "scaler_params": scaler_params,
            
            # ç¿»è¯‘æ–‡æœ¬
            "translations": translations,
            
            # å…ƒæ•°æ®
            "metadata": {
                **get_metadata(),
                "n_jobs": len(job_translations.get('en', [])),
                "n_questions": n_questions,
                "languages_available": list(job_translations.keys())
            },
            
            # åˆ†ç»„ä¿¡æ¯
            "norm_groups": {
                1: {"gender": "Female", "age_range": "<35", "description": "Female under 35"},
                2: {"gender": "Female", "age_range": ">=35", "description": "Female 35 and over"},
                3: {"gender": "Male", "age_range": "<35", "description": "Male under 35"},
                4: {"gender": "Male", "age_range": ">=35", "description": "Male 35 and over"}
            }
        }
        
        # 9. ä¿å­˜ä¸ºJSONæ–‡ä»¶
        print("\nSaving to JSON file...")
        output_file = "app_data_complete.json"
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(complete_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Successfully saved to {output_file}")
        print(f"ğŸ“ File size: {os.path.getsize(output_file) / 1024 / 1024:.2f} MB")
        
        # 10. æ‰“å°æ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“Š Data Summary:")
        print("=" * 60)
        
        n_jobs = len(complete_data.get('job_translations', {}).get('en', []))
        print(f"   â€¢ Total Jobs: {n_jobs}")
        print(f"   â€¢ Total Questions: {n_questions}")
        print(f"   â€¢ Languages Available: {len(complete_data.get('job_translations', {}))}")
        print(f"   â€¢ Job Features: {len(complete_data.get('scaled_features', []))}")
        print(f"   â€¢ Norm Groups: 4")
        print(f"   â€¢ Question Languages: {list(questions_dict.keys())}")
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        print("\nğŸ” Data Validation:")
        print(f"   â€¢ Job translations: {all(len(v) == n_jobs for v in complete_data.get('job_translations', {}).values())}")
        print(f"   â€¢ Job codes: {len(complete_data.get('job_codes', [])) == n_jobs}")
        print(f"   â€¢ Scaled features: {len(complete_data.get('scaled_features', [])) == n_jobs}")
        
        # æ˜¾ç¤ºç¤ºä¾‹
        print("\nğŸ‘€ Example Data (first 3 jobs in each language):")
        for lang in ['en', 'zh', 'es', 'fr', 'ru', 'ar']:
            if lang in complete_data.get('job_translations', {}):
                jobs = complete_data['job_translations'][lang][:3]
                print(f"   â€¢ {lang.upper()}: {jobs}")
        
        return output_file
        
    except Exception as e:
        print(f"\nâŒ Error during conversion: {e}")
        import traceback
        traceback.print_exc()
        return None

def create_compact_version():
    """åˆ›å»ºå‹ç¼©ç‰ˆæœ¬ï¼Œå‡å°‘æ–‡ä»¶å¤§å°"""
    print("\n" + "=" * 60)
    print("Creating compact version...")
    print("=" * 60)
    
    try:
        with open("app_data_complete.json", "r", encoding="utf-8") as f:
            full_data = json.load(f)
        
        # åˆ›å»ºå‹ç¼©ç‰ˆæœ¬ï¼Œç§»é™¤ä¸éœ€è¦çš„å­—æ®µ
        compact_data = {
            "job_translations": full_data.get("job_translations", {}),
            "job_codes": full_data.get("job_codes", []),
            "scaled_features": full_data.get("scaled_features", []),
            "pca_weights": full_data.get("pca_weights", []),
            "questions": full_data.get("questions", {}),
            "mean_norms": full_data.get("mean_norms", {}),
            "sd_norms": full_data.get("sd_norms", {}),
            "weights": full_data.get("weights", []),
            "scaler_params": full_data.get("scaler_params", {}),
            "translations": {
                "trait_names": full_data.get("translations", {}).get("trait_names", {}),
                "disclaimer": full_data.get("translations", {}).get("disclaimer", {})
            },
            "metadata": {
                "n_jobs": len(full_data.get("job_translations", {}).get("en", [])),
                "n_questions": len(full_data.get("questions", {}).get("en", [])),
                "languages": list(full_data.get("job_translations", {}).keys())
            }
        }
        
        with open("app_data_compact.json", "w", encoding="utf-8") as f:
            json.dump(compact_data, f, ensure_ascii=False, separators=(',', ':'))
        
        print(f"âœ… Compact version saved to app_data_compact.json")
        print(f"ğŸ“ File size: {os.path.getsize('app_data_compact.json') / 1024:.2f} KB")
        
    except Exception as e:
        print(f"âŒ Failed to create compact version: {e}")

if __name__ == "__main__":
    # è½¬æ¢å®Œæ•´æ•°æ®
    output_file = convert_all_data()
    
    if output_file:
        # åˆ›å»ºå‹ç¼©ç‰ˆæœ¬
        create_compact_version()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ All conversions complete!")
        print("=" * 60)
        
        print("\nğŸ“‚ Generated files:")
        print("  â€¢ app_data_complete.json - Complete dataset with all translations")
        print("  â€¢ app_data_compact.json  - Compact version (smaller file size)")
        
        print("\nğŸš€ Next steps:")
        print("1. Update your HTML to load the correct JSON file:")
        print("""
   // In your HTML/JS, change the loading code:
   fetch('app_data_complete.json')  // or 'app_data_compact.json'
     .then(response => response.json())
     .then(data => {
       appData = data;
       console.log('Loaded', data.metadata.n_jobs, 'jobs in', 
                   data.metadata.languages.length, 'languages');
     });
        """)
        
        print("\n2. Update your JavaScript to use multilingual job names:")
        print("""
   // When displaying job recommendations:
   function getJobName(jobIndex, language) {
     if (appData.job_translations && appData.job_translations[language]) {
       return appData.job_translations[language][jobIndex];
     }
     return appData.job_names[jobIndex]; // fallback
   }
        """)
    else:
        print("\nâŒ Conversion failed. Please check the error messages above.")